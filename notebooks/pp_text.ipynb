{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Van Gysel et al also prune to top 2^16 most frequent words which also allows them to fit in uint16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from functools import partial\n",
    "import nltk\n",
    "from string import punctuation\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "PATH_DATA = '../data/amazon/food/reviews_df.msg'\n",
    "stop = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_msgpack(PATH_DATA)#.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 52s, sys: 888 ms, total: 12min 52s\n",
      "Wall time: 12min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "purge_words = stop + list(punctuation) + ['\"', '\\'', '\\\\']\n",
    "data_words = df.Text\\\n",
    "    .apply(lambda row: [stemmer.stem(word.lower())\n",
    "                        for word in nltk.word_tokenize(row) \n",
    "                        if word.lower() not in purge_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.83 s, sys: 8 ms, total: 4.84 s\n",
      "Wall time: 4.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Find most common words\n",
    "c = Counter()\n",
    "_ = data_words.map(lambda words: c.update(words))\n",
    "thresh = 2**15-1  # should be 2**16 but I want less\n",
    "freq_words, _ = zip(*c.most_common(thresh))\n",
    "freq_words = set(freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 52s, sys: 548 ms, total: 13min 53s\n",
      "Wall time: 13min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Go through again and filter out uncommon words\n",
    "data_words = data_words\\\n",
    "    .map(lambda words: [word for word in words if word in freq_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set(it.chain(*data_words))\n",
    "vocab_size = len(vocab) + 1  # +1 for UNK\n",
    "\n",
    "max_len = max(map(len, data_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40 ms, sys: 0 ns, total: 40 ms\n",
      "Wall time: 40.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "le = LabelEncoder()\n",
    "le.fit(list(vocab))\n",
    "le.classes_ = np.insert(le.classes_, 0, '')  # UNK tok @ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_d = dict(zip(le.classes_, range(len(le.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.53 s, sys: 52 ms, total: 4.58 s\n",
      "Wall time: 4.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_words_enc = data_words.map(lambda l: list(map(enc_d.get, l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_words_enc.to_msgpack('../data/amazon/food/reviews_txt_enc_s.msg', compress='blosc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(list(le.classes_), open('../data/amazon/food/vocab.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
