{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import itertools as it\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "from time import time\n",
    "\n",
    "LOG_DIR = '/tmp/tensorboard-logs/semantic/'\n",
    "\n",
    "PATH_DATA = '../data/amazon/food/reviews_df.msg'\n",
    "PATH_ENC_TXT = '../data/amazon/food/reviews_txt_enc_s.msg'\n",
    "PATH_VOCAB = '../data/amazon/food/vocab.p'\n",
    "\n",
    "entity_col = 'ProductId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "word_emb_size = 64\n",
    "entity_emb_size = 16\n",
    "n_negs_per_pos = 10  # number of negatives to sample per positive\n",
    "\n",
    "adam_alpha = 0.001\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "l2_emb = 1e-2  # for embeddings\n",
    "l2_map = 1e-2  # for mapping matrices\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_msgpack(PATH_DATA)\n",
    "data_words_enc = pd.read_msgpack('../data/amazon/food/reviews_txt_enc_s.msg')\n",
    "vocab = pickle.load(open(PATH_VOCAB, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_entities = len(df[entity_col].cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('reg'):\n",
    "    reg_emb = tf.contrib.layers.l2_regularizer(l2_emb)\n",
    "    reg_map = tf.contrib.layers.l2_regularizer(l2_map)\n",
    "    \n",
    "with tf.variable_scope('emb'):\n",
    "    word_embs = tf.get_variable(\n",
    "        name='word',\n",
    "        shape=(len(vocab), word_emb_size),\n",
    "        initializer=None,  # use default glorot\n",
    "        regularizer=reg_emb\n",
    "    )\n",
    "    entity_embs = tf.get_variable(\n",
    "        name='item',\n",
    "        shape=(n_entities, entity_emb_size),\n",
    "        initializer=None,  # use default glorot\n",
    "        regularizer=reg_emb\n",
    "    )\n",
    "    \n",
    "with tf.variable_scope('ph'):\n",
    "    ngram_ph = tf.sparse_placeholder(tf.int32)\n",
    "    pos_entity_ph = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    neg_entities_ph = tf.placeholder(tf.int32, shape=[batch_size, n_negs_per_pos])\n",
    "\n",
    "with tf.variable_scope('looked'):\n",
    "    agg_looked_word_emb = tf.nn.embedding_lookup_sparse(\n",
    "        word_embs, ngram_ph, None, combiner='mean')\n",
    "    pos_looked_entity_emb = tf.nn.embedding_lookup(entity_embs, pos_entity_ph)\n",
    "    neg_looked_entities_emb = tf.nn.embedding_lookup(entity_embs, neg_entities_ph)\n",
    "    \n",
    "f = tf.contrib.layers.fully_connected(\n",
    "    inputs=agg_looked_word_emb,\n",
    "    num_outputs=entity_emb_size,\n",
    "    activation_fn=tf.nn.tanh,\n",
    "    # use default xavier/glorot init\n",
    "    weights_regularizer=reg_map,\n",
    "    scope='map',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_score = tf.sigmoid(\n",
    "    tf.reduce_sum(\n",
    "        tf.multiply(f, tf.squeeze(pos_looked_entity_emb)),\n",
    "        axis=-1, keep_dims=False),\n",
    "    name='pos_score')\n",
    "neg_scores = tf.sigmoid(\n",
    "    tf.reduce_sum(\n",
    "        tf.multiply(\n",
    "            tf.reshape(tf.tile(f, tf.constant([n_negs_per_pos, 1])), shape=[batch_size, n_negs_per_pos, entity_emb_size]),\n",
    "            neg_looked_entities_emb),\n",
    "        axis=-1, keep_dims=False),\n",
    "    name='neg_scores')\n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "    tf.log(pos_score) + tf.reduce_sum(tf.log(1. - neg_scores), axis=-1),\n",
    "    name='loss_mnce'\n",
    ")\n",
    "\n",
    "loss_reg = sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "loss_tot = loss + loss_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate=adam_alpha, beta1=adam_beta1, beta2=adam_beta2)\n",
    "\n",
    "global_step = tf.get_variable(\n",
    "    'global_step', shape=[], trainable=False,\n",
    "    initializer=tf.constant_initializer(0))\n",
    "\n",
    "train_op = opt.minimize(loss_tot, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 s, sys: 632 ms, total: 15.3 s\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rows, cols = zip(*it.chain(\n",
    "    *([(row, col) for col in cols] \n",
    "      for row, cols in enumerate(data_words_enc))))\n",
    "indices = np.array(rows)[:, None]\n",
    "vals = np.array(cols, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_codes = df[entity_col].cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_enc_csr = sp.csc_matrix(\n",
    "    (np.ones(len(rows)), (rows, cols)),\n",
    "     shape=(len(data_words_enc), len(vocab)),\n",
    "     dtype=bool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shit_full_gen():\n",
    "    while True:\n",
    "        # note: actually we should be uniformly sampling over entities rather than documents\n",
    "        inds = np.random.permutation(np.arange(len(data_words_enc)))\n",
    "\n",
    "        for ii in range(0, len(inds)-batch_size+1, batch_size):\n",
    "            batch_inds = inds[ii:ii+batch_size]\n",
    "\n",
    "            batch_pos_entity_codes = entity_codes[batch_inds, None]\n",
    "            batch_neg_entity_codes = np.random.randint(\n",
    "                0, n_entities,size=[batch_size, n_negs_per_pos])\n",
    "\n",
    "            # Note: slicing rows of sp matrix is SLOW\n",
    "            batch_words_rows, batch_words_cols = data_enc_csr[batch_inds].nonzero()\n",
    "\n",
    "            # Note: we are supposed to grab a window of words here\n",
    "            #     instead of the entire doc\n",
    "            batch_words_sptv = tf.SparseTensorValue(\n",
    "                batch_words_rows[:, None],\n",
    "                batch_words_cols,\n",
    "                dense_shape=[batch_size])\n",
    "\n",
    "            feed_d = {\n",
    "                ngram_ph: batch_words_sptv,\n",
    "                pos_entity_ph: batch_pos_entity_codes,\n",
    "                neg_entities_ph: batch_neg_entity_codes,\n",
    "            }\n",
    "\n",
    "            yield feed_d\n",
    "        \n",
    "def get_win(seq, win_size=4):\n",
    "    ii = np.random.randint(0, max(0, len(seq) - win_size)+1)\n",
    "    return seq[ii:ii+win_size]\n",
    "        \n",
    "def shit_win_gen():\n",
    "    # note: actually we should be uniformly sampling over entities rather than documents\n",
    "    while True:\n",
    "        inds = np.random.permutation(np.arange(len(data_words_enc)))\n",
    "\n",
    "        for ii in range(0, len(inds)-batch_size+1, batch_size):\n",
    "            batch_inds = inds[ii:ii+batch_size]\n",
    "\n",
    "            batch_pos_entity_codes = entity_codes[batch_inds, None]\n",
    "            batch_neg_entity_codes = np.random.randint(\n",
    "                0, n_entities,size=[batch_size, n_negs_per_pos])\n",
    "\n",
    "            batch_words = data_words_enc.iloc[batch_inds].map(get_win)\n",
    "            batch_words_rows, batch_words_cols = zip(*it.chain(\n",
    "                *([(row, col) for col in cols] \n",
    "                  for row, cols in enumerate(batch_words))))\n",
    "\n",
    "            # Note: we are supposed to grab a window of words here\n",
    "            #     instead of the entire doc\n",
    "            batch_words_sptv = tf.SparseTensorValue(\n",
    "                np.array(batch_words_rows)[:, None],\n",
    "                np.array(batch_words_cols, dtype='int32'),\n",
    "                dense_shape=[batch_size])\n",
    "\n",
    "            feed_d = {\n",
    "                ngram_ph: batch_words_sptv,\n",
    "                pos_entity_ph: batch_pos_entity_codes,\n",
    "                neg_entities_ph: batch_neg_entity_codes,\n",
    "            }\n",
    "\n",
    "            yield feed_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = shit_win_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proj_config = projector.ProjectorConfig()\n",
    "\n",
    "word_proj = proj_config.embeddings.add()\n",
    "word_proj.tensor_name = word_embs.name\n",
    "word_proj.metadata_path = os.path.join(LOG_DIR, 'word_metadata.tsv')\n",
    "\n",
    "# single column meta does not have header\n",
    "pd.Series(list(enumerate(vocab))).to_csv(os.path.join(LOG_DIR, 'word_metadata.tsv'), sep='\\t', index=False, header=False)\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx # epochs: 180.13770683291875\n",
      "0 0.04048943519592285\n",
      "1000 14.889153957366943\n",
      "2000 14.643097400665283\n",
      "3000 14.770846843719482\n",
      "4000 15.402669191360474\n",
      "5000 14.724296569824219\n",
      "6000 14.788334369659424\n",
      "7000 14.598264217376709\n",
      "8000 14.37799334526062\n",
      "9000 14.463687419891357\n",
      "10000 14.65018343925476\n",
      "11000 14.393420457839966\n",
      "12000 14.730337619781494\n",
      "13000 14.991628646850586\n",
      "14000 14.717162370681763\n",
      "15000 14.440537929534912\n",
      "16000 14.651312351226807\n",
      "17000 14.377591848373413\n",
      "18000 14.44415283203125\n",
      "19000 14.670193910598755\n",
      "20000 14.617869853973389\n",
      "21000 14.42133355140686\n",
      "22000 14.773431062698364\n",
      "23000 14.458477020263672\n",
      "24000 14.347233057022095\n",
      "25000 14.587358713150024\n",
      "26000 14.323538541793823\n",
      "27000 14.316344976425171\n",
      "28000 14.552619934082031\n",
      "29000 14.405521392822266\n",
      "30000 14.19519853591919\n",
      "31000 14.659892797470093\n",
      "32000 14.527189493179321\n",
      "33000 14.35249137878418\n",
      "34000 14.695753574371338\n",
      "35000 14.4535973072052\n",
      "36000 14.4223792552948\n",
      "37000 14.64562702178955\n",
      "38000 14.465288400650024\n",
      "39000 14.200994968414307\n",
      "40000 14.617127180099487\n",
      "41000 14.299070835113525\n",
      "42000 14.175500392913818\n",
      "43000 14.436546564102173\n",
      "44000 14.342857360839844\n",
      "45000 14.279407501220703\n",
      "46000 14.511404275894165\n",
      "47000 14.231868505477905\n",
      "48000 14.297338962554932\n",
      "49000 14.58349084854126\n",
      "50000 14.326192140579224\n",
      "51000 14.138744831085205\n",
      "52000 14.634443759918213\n",
      "53000 14.240421295166016\n",
      "54000 14.249927282333374\n",
      "55000 14.527123212814331\n",
      "56000 14.248059511184692\n",
      "57000 14.268661737442017\n",
      "58000 14.482770442962646\n",
      "59000 14.298831939697266\n",
      "60000 14.17955231666565\n",
      "61000 14.474443674087524\n",
      "62000 14.269845485687256\n",
      "63000 14.103737115859985\n",
      "64000 14.613116025924683\n",
      "65000 14.391910552978516\n",
      "66000 14.223773002624512\n",
      "67000 14.540796518325806\n",
      "68000 14.30440902709961\n",
      "69000 14.316011667251587\n",
      "70000 14.594781637191772\n",
      "71000 15.200884103775024\n",
      "72000 14.261124610900879\n",
      "73000 14.6482412815094\n",
      "74000 14.507949352264404\n",
      "75000 14.449981451034546\n",
      "76000 15.051243782043457\n",
      "77000 14.516138315200806\n",
      "78000 14.31092643737793\n",
      "79000 14.717354536056519\n",
      "80000 14.403217315673828\n",
      "81000 14.273308277130127\n",
      "82000 14.554395198822021\n",
      "83000 14.407730102539062\n",
      "84000 14.750526905059814\n",
      "85000 14.807966470718384\n",
      "86000 14.388762950897217\n",
      "87000 14.169378519058228\n",
      "88000 14.705942630767822\n",
      "89000 14.313075542449951\n",
      "90000 14.178681373596191\n",
      "91000 14.657262325286865\n",
      "92000 14.412286520004272\n",
      "93000 14.531537055969238\n",
      "94000 15.092179298400879\n",
      "95000 14.374126195907593\n",
      "96000 14.270043849945068\n",
      "97000 14.618940830230713\n",
      "98000 14.350590467453003\n",
      "99000 14.407896518707275\n"
     ]
    }
   ],
   "source": [
    "max_steps = 100000\n",
    "\n",
    "print(f'Approx # epochs: {max_steps*batch_size/len(df)}')\n",
    "\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "saver = tf.train.Saver()\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tic = time()\n",
    "    for step in range(max_steps):\n",
    "        feed = next(gen)\n",
    "        sess.run(train_op, feed_dict=feed)\n",
    "        \n",
    "        if (step%1000) == 0:\n",
    "            toc = time() - tic\n",
    "            print(step, toc)\n",
    "            tic = time()\n",
    "\n",
    "            saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"), step)\n",
    "            projector.visualize_embeddings(summary_writer, proj_config)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-30d9529ff51c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(list(enumerate(vocab))).to_csv(os.path.join(LOG_DIR, 'word_metadata.tsv'), sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fb8e652f518>: SparseTensorValue(indices=array([[   0],\n",
       "       [   0],\n",
       "       [   0],\n",
       "       ..., \n",
       "       [1023],\n",
       "       [1023],\n",
       "       [1023]]), values=array([17970, 13654, 28557, ..., 21822, 24132, 31921], dtype=int32), dense_shape=[1024]),\n",
       " <tf.Tensor 'ph/Placeholder_3:0' shape=(1024, 1) dtype=int32>: array([[24607],\n",
       "        [ 5927],\n",
       "        [69858],\n",
       "        ..., \n",
       "        [47834],\n",
       "        [40309],\n",
       "        [71170]], dtype=int32),\n",
       " <tf.Tensor 'ph/Placeholder_4:0' shape=(1024, 10) dtype=int32>: array([[56344, 44833, 25306, ...,  6432,  7376, 70702],\n",
       "        [26528, 69461, 31499, ..., 24419, 23698, 13890],\n",
       "        [63025,  5350, 33143, ..., 18762,  2815, 32479],\n",
       "        ..., \n",
       "        [55719, 72111, 71627, ..., 63892, 39498, 38823],\n",
       "        [ 5972, 59571, 71122, ..., 16453, 68166,  5105],\n",
       "        [  264, 66604, 48647, ..., 25273,  9870, 47331]])}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['littl', 'generous', 'sweeter'],\n",
       "      dtype='<U81')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(vocab)[[17970, 13654, 28557]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductId                                                        B0047E2I5U\n",
       "UserId                                                       A1EMMC2NCSXPSW\n",
       "ProfileName                                        Debra D. Laflen \"gr8skn\"\n",
       "HelpfulnessNumerator                                                      2\n",
       "HelpfulnessDenominator                                                    2\n",
       "Score                                                                     5\n",
       "Time                                                             1281052800\n",
       "Summary                                                      great dog food\n",
       "Text                      My dog Denali loves the Ziwi Peak food.  It is...\n",
       "Name: 24608, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[24607]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'Love these beans! The chocolate coating is creamy and smooth, and a little more generous than most. It has a sweeter taste than the coating on some other varieties (sweet, but not cloying). That balances well with the sharp flavor of the espresso bean. The appearance of these is pleasing also. There is an even distribution of dark chocolate, white chocolate, and a pretty speckled milk-on-white that conceals a milk chocolate underlayer.<br /><br />I like the presentation of Dilettante Chocolates; on the label there are two short paragraphs about the history and practices of the company that incorporate that information charmingly and with a concise flair that is very modern.<br /><br />And the price for this somewhat decadent purchase, three pounds of chocolate covered espresso beans, is terrific even with shipping. Service was very good too. I received my order within three business days of placing it.',\n",
       "       'Seattle Gourmet Foods is a company I found through Amazon.com The past few years I have found myself ordering more and more on the internet.I always skipped the reviews because I just thought they were probably written by people who benefited from my purchase. However, I have become very dependent on what my fellow shoppers have to say,and really surprised when I realized Amazon allow customers comments good and bad to be printed.<br />Anyway that being said I will tell you this product came and was delivered as promised.There are so many in this barrel shaped container it almost seems bottomless.The chocolate covered gems are so rich and creamy with just the right amount of chocolate covering the crunchy fresh expresso bean in the middle.Just a few will do you packing a delicious caffein buzz! I thought they were a bit expensive when I first placed my order, hoping the saying \"you get what you pay for\" was true I orderd anyway and am very pleased with the value. Getting what I paid for and a whole lot more.<br /><br />Thanks from one very satisfied customer on Morgantown Pa.<br />Karen',\n",
       "       \"These can be very addictive.<br /><br />By far, these are the best chocolate-covered espresso beans I've ever tried.  Dillettante knows what we love.\",\n",
       "       \"I purchased 2 of the 5lb bags (hoping for the best of course - better deal on shipping if you buy more then 1 bag.  This was a blind purchase as I have never tried this brand before.<br /><br />I was and am very impressed with the quality of this product, I have tried several different brands from amazon as well as from local coffee shops of choco covered espresso beans, I got all the ladies addicted to these things with my last purchse from a different seller/chocolatier so I was buying for 4 for us to split.  To say the very least everyone is very satisfied and they can't comment as they have their mouths full of chocolate/caffein goodness at the moment while running up and down the aisles at work;)<br /><br />One review did say these had espresso bits which is false, as soon as I read that review I took half the chocolate off and yes.. there is in fact 1 intact bean deep inside the chocolate.  The chocolate to bean ratio is a little much but since the quality of the chocolate is so good it is no problem what so ever! The quality is there, great buy and trust me buy multiple of the 5lb bags and share/split with friends (or keep them all I know you'll love them)<br /><br />Thank you.. Love them buying again!!!!\",\n",
       "       'These beans were wonderful.  My customers raved over them.  The chocolate is amazing!',\n",
       "       \"These are okay, but the ratio of chocolate to bean is too much for me, and even the dark chocolate taste too sweet.  It could just be the batch I got, but the espresso bean didn't stand out as much as some other brands I've tried (even eating the bean alone), and it is overwhelmed by the chocolate taste.\",\n",
       "       'These chocolate covered espresso beans are absolutely heavenly.  Just enough caffeine to awaken you in the afternoon--preferably after lunch.',\n",
       "       'Loved them, they were awesome! I recieved them to an APO AE address 1 week after ordering. I have never had a package get here that fast.<br /><br />Only downer is that the chocolate is very rich (not such a bad thing) and its alot of chocolate for a little bean',\n",
       "       'I bought a 10oz package of these locally (Walmart) after seeing these on amazon but balking because of the shipping.  The chocolate is nice and creamy while the roasted coffee bean gives a nice smokey/crispy bite.  These make a great snack and are not too sweet.',\n",
       "       'Have the container of these sitting out in the open and any time anyone walks in they grab a handful.  They are very addicting, easy 5/5 stars.',\n",
       "       'These chocolate espresso beans were the very best I have ever had.  They were a birthday gift for my mother, who is something of a connouisseur of these things, and she rated them absolutely tops.  After my brother sampled them, he immediately ordered a supply for himself.',\n",
       "       \"I received the chocolate espresso beans as a gift and they came in great condition. I leave them on a counter, or put them in a ziplock in my bookbag and they don't melt easily. They taste very good and get the job done for an extra perk.\",\n",
       "       \"All I can say is that these are very addictive, by that I mean it's difficult to eat just one. They have a wonderful flavor and really satisfy the taste buds.\",\n",
       "       'the best chocolate covered coffee bean i have ever had. if you enjoy coffee and chocolate i insist that you try this product',\n",
       "       'My husband and I ordered these for our wedding reception and they were a hit!  Everyone loved them and they arrived quicker then I had anticipated!',\n",
       "       'These beans are really good and the three types of chocolate are fun.  I like these better than the ones sold at Trader Joes, although they are more expensive.',\n",
       "       \"I wasn't too impressed with these espresso beans. Unfortunately, I had a 3lb tub to deal with!<br />I've had very smooth and decadent espresso beans, but there is something very off-putting about the taste of these Dilettante ones. The white chocolate is supppppper sweet and the espresso beans inside tasted very hard and sharp. Wouldn't buy again.\",\n",
       "       'I bought these as gifts and was dismayed to discover that the description on Amazon very prominently and clearly describes these as \"chocolate covered espresso beans\", but this description appeared nowhere on the package itself. On the package, in BIG letters reads: ESPRESSO CHOCOLATE BLEND.<br /><br />I asked the people who I gifted these to what they were like and they told me while they were tasty, they were basically three different kinds of chocolate with crunchy bits of espresso bean mixed in.<br /><br />If you want an espresso/chocolate \"confection\", these are for you.  If you are looking for chocolate covered espresso beans, look elsewhere.'], dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[np.where(entity_codes == 24607)[0]].Text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'emb/word:0' shape=(32768, 64) dtype=float32_ref>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
